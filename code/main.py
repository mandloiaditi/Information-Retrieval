# -*- coding: utf-8 -*-
"""IR1Aditi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18bMFDnjGZM_bp0VxPXB7P1POe6w4EUV9

# Text Preprocessing Using Python

author:  Aditi Mandloi 
created:  Jan 24, 2020 

"""

# from google.colab import drive
# drive.mount('/gdrive')

import nltk
import re
# import os
import sys
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.util import ngrams
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer,WordNetLemmatizer
from collections import Counter,defaultdict
import collections 
import matplotlib.pyplot as plt
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# %cd /gdrive
# os.chdir("./My Drive")

def read_file(filename):
  '''
  Function to parse the file into clean text without any html 
  tags using regex"
  our corpus has tags such as '<doc>','<\doc>','<a .*>','<\a>' and '<br>'
  '''
  try:
    with open(filename, 'r') as file:
      data = file.read()
  except IOError:
  		print("Error: File does not appear to exist.")
  		exit()
  # incase we want to store all documents separately
  # rex = re.compile(r'<doc.*?>(.*?)</doc>',re.S|re.M)
  # docs  = [doc.group(1).strip() for doc in re.finditer(r'<doc.*?>(.*?)</doc>',data,flags = re.M|re.S)]
  text = re.sub(r"([^>]*)<.*?>(.*?)", r"\1\2", data,flags = re.M|re.S)
  return text

"""## TOKENIZATION USING NLTK"""

def tokeniser(text):
  '''
  Function to tokenize text to extract words without punctuation 
  using regex utility of nltk tokenizer.

  The regular expression removes all punctuations and special characters(except those 
  associated with digits)

  example input string: "Hello Adam/Wallace U.S.Mexico (930984) A-31,670+ ₹89374 M.R.P. 9898 19-12-1929 then-double>?"

  ouput tokens:  ['Hello', 'Adam', 'Wallace', 'U.S.', 'Mexico', '930984', 'A-31,670', '₹89374', 'M.R.P.', '9898',
   '19-12-1929', 'then', 'double']

  while tokenizing removes any punctuations or special characters except those prefixed to a digit ($,₹ etc)
  '''
  # tokens = nltk.word_tokenize(text)
  tokenizer = nltk.RegexpTokenizer('[\d]*[^\r\n\t\s[{(]*[\d]+[^\r\n\t\s[{(]*|[A-Z][a-zA-Z\.{1}]*[A-Z]\.|\w+')
  tokens = tokenizer.tokenize(text)
  
  return tokens

def fq_log_plot(fdist, fig_name, log_scale=True,cumulative = False, max_num = 40):
  '''
  Function to plot line-plots (for top 40/max_num most frquent n-grams) and Frequency plot for all tokens on a log scale
  '''
  fdist_sorted = (sorted(fdist.items(), key= lambda x: x[1]))[::-1]
  n = len(fdist_sorted[0][0])

  if(log_scale == False):
    fig = plt.figure(figsize = (12,8))
    title = "WORD FREQUENCIES ON LINEAR SCALE(TOP "+ str(max_num) +" NGRAMS)"
    fdist.plot(max_num, cumulative=cumulative,title= title, linewidth=2,color='g')
    fig_name = fig_name + str(n)+ '-gram_freqDist.png'
    fig.savefig(fig_name, bbox_inches = "tight")
    return
  
  index = 1
  rank = []
  count = []
  for freq in fdist_sorted:
    rank.append(index)
    count.append(fdist_sorted[index-1][1])
    index +=1
  fig = plt.figure(figsize = (12,8))
  plt.grid()
  plt.rc('xtick',labelsize=15)
  plt.rc('ytick',labelsize=15)
  plt.xlabel("Rank of "+ str(n) +"-gram in sorted count dictionary",fontsize=20)
  plt.ylabel("Total Number of Occurrences",fontsize=20)
  plt.setp(plt.title("WORD FREQUENCIES ON LOG SCALE",fontsize=23), color='black') 
  plt.loglog(rank, count, basex=10,color='red')
  fig_name = fig_name + str(n)+ '-gram_freqDist(Log).png'
  fig.savefig(fig_name, bbox_inches = "tight")
  

def freq_percent(fd, percent):
    total_sum = sum(fd.values())
    fd_sorted = (sorted(fd.items(), key= lambda x: x[1] ))[::-1]
    n = len(fd_sorted[0][0])
    pos = fd_sorted[0][1]  # sum of frequencies upto ind
    ind = 1  # current position in fd_sorted
    percent_pos = (percent *total_sum )/100.0
    while pos <= percent_pos and ind < len(fd_sorted):
        pos += fd_sorted[ind][1]
        ind += 1
    print(str(percent) + "% of the corpus can be covered by " + str(ind) +" " +str(n)+"-grams")
    return (fd_sorted[ind][0],ind)

"""## NGRAMS ANALYSIS AFTER TOKENIZATION"""

def ngrams_analysis(tokens,post_lem=False,post_stem=False, plot_graph = True):
  ''' 
  Function to plot and save frequency graphs for unigram,
  bigram and trigrams for given sequence of tokens.
  Prints required tokens to cover given percentage of corpus
  '''
  for i in range(1,4): #1=UNIGRAM , 2=BIGRAM, 3=TRIGRAM
    kgrams = ngrams(tokens,i)
    percent = (10-i)*10 # 90% 80% 70% coverage for uni-,bi- and tri-grams respectively 
    fd = nltk.FreqDist(kgrams)
    x = (sorted(fd.items(), key= lambda x: x[1]))[::-1]
    print("\nTotal number of unique " + str(i) + "-grams are: " + str(len(x)))
    freq_percent(fd,percent)
    if(post_lem):
      fig_name = "Lemmatized"
    elif(post_stem):
      fig_name = "Stemmed"
    else:
      fig_name = ""
    print("\nFor checking zipf's law: ")
    print("count of most common " + str(i)+"-gram (rank 1): ",x[0][1])
    print("count of second common "+str(i)+"-gram (rank 2): ",x[1][1])
    print("count of third common "+ str(i)+"-gram (rank 3): ",x[2][1])
    print("count of fourth common "+ str(i)+"-gram  (rank 4): ",x[3][1])
    if(plot_graph):
      fq_log_plot(fd,fig_name,log_scale=False)
      fq_log_plot(fd,fig_name,log_scale=True)

"""## NGRAM ANALYSIS AFTER STEMMING"""

def stemming_analysis_ngrams(tokens ,plot_graph = True):
  ps = PorterStemmer()
  stemmed_tokens = [ps.stem(word) for word in tokens]
  ngrams_analysis(stemmed_tokens ,post_lem=False,post_stem=True, plot_graph = plot_graph)

"""## NGRAM ANALYSIS AFTER LEMMETIZATION"""

def lemmatization_analysis_ngrams(tokens, plot_graph = True):
  lemmatizer = WordNetLemmatizer()
  # Creating part of speech tags for better results (lemmatization)
  tags = defaultdict(lambda : wordnet.NOUN)
  tags['R'] = wordnet.ADV
  tags['V'] = wordnet.VERB
  tags['N'] = wordnet.NOUN
  tags['J'] = wordnet.ADJ
  lem_tokens = [lemmatizer.lemmatize(token, tags[tag[0]]) for token, tag in nltk.pos_tag(tokens)]
  ngrams_analysis(lem_tokens ,post_lem=True,post_stem=False,plot_graph = plot_graph)

def chisqr_collocation(tokens, topx , cut_off):
  collocations = []
  bigrams = list(ngrams(tokens,2))
  unigrams = list(ngrams(tokens,1))
  fdist1 = nltk.FreqDist(tokens)
  fdist2 = nltk.FreqDist(bigrams)
  total = len(bigrams)
  '''         W1    ~W1
             -----------
         W2 | c1  |  c2 | = count_1
             ----------
        ~W2 | c3  |  c4 |
             ----------
             = count_0     = total
  '''
  for pair in fdist2.keys():
    c1 = fdist2[pair] # both word
    if(c1 > cut_off): #defining a cut off frequency
      count_1 = fdist1[pair[1]]
      count_0 = fdist1[pair[0]]

      # Calculation of bigram association measure:
      c2 = count_1 - c1 # second but not first word bigrams 
      c3 = count_0 - c1 # first but not second word bigrams
      c4 = total-c1-c2-c3 # not first not second
      x  = (c1*c4 - c2*c3)**2
      y = (c1 + c2)*(c1 + c3)*(c3+c4)*(c2+c4)
      res  = total * x/y
      collocations.append((res,pair))

  collocations = (sorted(collocations, key= lambda x: x[0]))[::-1] #in descending order
  return [ ' '.join(pair) for score,pair in collocations[0:topx]]

def main():
  if(len(sys.argv) > 1):
    filename = sys.argv[1]
  else:
    filename = "wiki_22"
  text  = read_file(filename)
  tokens =  tokeniser(text)
  if(len(sys.argv) > 2 and sys.argv[2] == '1'):
    plot_graph = True
  else:
    plot_graph = False
  print("\n\n*******N-GRAM Analysis before stemming and tokenization*******\n\n\n")
  ngrams_analysis(tokens , plot_graph=plot_graph)
  # after stemming
  print("\n\n\n\n****NGRAM ANALYSIS FOR N = 1,2,3 AFTER STEMMING****\n\n\n")
  stemming_analysis_ngrams(tokens , plot_graph =plot_graph)
  # after lemmatization
  print("\n\n\n\n****NGRAM ANALYSIS FOR N = 1,2,3 AFTER LEMMATIZATION****\n\n\n")
  lemmatization_analysis_ngrams(tokens , plot_graph = plot_graph)
  # Top - 20 Collocayions using chi-square test
  print("Top 20 collocations found are:")
  print(chisqr_collocation(tokens,topx = 20,cut_off= 25))
  

if __name__ == "__main__":
  main()
